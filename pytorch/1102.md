# 第二章 加载数据  
- 为了保证不爆内存，需要用到data_set data_loader  
- 在上一个项目中,是把所有的数据都读进去。但是当数据量特别大时,很难放进去所有的数据。  
- 这时候就要用到SGD。  
- 通过读取噪声可以躲过鞍点。  
- 采用SGD是一组一组的训练,会比较满,但是不容易被鞍点卡住。  
### 名词解释
-  epoch:训练的一个循环,所有的东西训练一次叫一个epoch。  
-  batch size:一次训练的一小批的数量  
-  iteration:总数据除以一个batch size  
### dataloader  
有两个参数,第一个是batch size,第二个参数是shuffle,即是否“洗牌”。  
```python
import torch  
from torch.utils.data import dataset
from torch.utils.data import dataloader

class DiabetesDataset(dataset):
	def __init__(self):
		pass

	def _getitem__(self, index):##可以给数据集编号
		pass

	def __len__(self):##返回数据集的数据个数
		pass

dataset = DiabetesDataset()
##初始化数据,其中num_works代表线程数,不要超过电脑核数
train_loader = dataloader(dataset = dataset, batch_size = 32, shuffle = True, num_workers = 2)
```

## 多分类问题  
为了解决“输入是某一类的概率”的问题，需要引入softmax层。计算概率分布的方法是，将每一个最后一层的结果求$e^n$,再求各个的占比。即：$\frac{e^{n1}}{e^{n1}+e^{n2}+e^{n3}}$.最后该结果在哪一类里的概率最大，就会被判定为哪一类。